*** Processes Recap ***
	A process has several ingredients. One of them is virtual address space that is used to allocate memory and use memory in general in the context of this process. We know that most processes are based on some executable image, which is the EXE file containing the initial code and data to run the process. Every process has its own handle table, which is used to access various kernel objects, and kernel objects, or executive objects, are used in Windows in many, many many ways. Depending on the type of object, they allow us to get functionality out of Windows. If you have a file object, for instance, we can access a file or a device. If we have a registry key object, we can access some information in the registry and so one. So every process has its own private handle table.

	Every process has a primary token. This is the access token, the security context under which all the threads in the process execute by default. And then, of course, we have threads. Threads are necessary in order to actually execute code. We know that the process is in itself a management object and doesn't really execute any code, it doesn't really mean anything. Threads are the actual entities running code and that's why when the process is created, one thread is created with it in a typical way. And then a thread can also perform something called impersonation and have its own token known as an impersonation token.


*** Process Creation ***
	So we have some things that have to happen in the context of the creator process, the process that calls the CreateProcess API to create a new process or perhaps one of the other process creation APIs. And some of that initialization happens in the context of this process because at this point the new process is not yet ready to do any running of any kind of code.

	And then later on some of the initialization of the new process has to happen in the context of that new process. So let's start with a creator process calling a functions such as CreateProcess. The first thing that has to happen is that the kernel has to open the image file, the portable executable file, and make sure that it is in fact a valid P file and contains the basic ingredients that the P file should contain. Assuming everyting is ok, it's going to create the kernel objects that manage a process.

	This is the executive process object and within it the kernel process object, which we'll look at later on in this module. And then it creates the thread object as well because we know that when a process is created, the first thread is created with it, and that's the thread that eventually will run the main function of that process, the one that's coming from the image file. So we have an executable thread object and a kernel thread object that are created as part of this initialization step. And then the image itself, the executable, and NTDLL are mapped into the address space of the newly created process. This is necessary because nobody else can actually do it. So the image itself is necessary because the data and the code is there, and then NTDLL has the special role, which is in this case called the loader, and its purpose here is to provide some initialization in the context of the new process. So there's nobody else that can make that kind of mapping. We'll see that later on the other DLLs that the process might need are going to be mapped or atleast loaded by NTDLL itself. But someone has to map NTDLL, and so the kernel does that explicitly.

	So the first thing that we have in terms of mappings in the new process is always or are always the executable and NTDLL. Assuming we're talking about the process creation that belongs to the Windows subsystem, which is the most common case, I'm not talking about native process creation at this point, then CSRSS, the Windows subsystem process, needs to know about the fact that a new process and thread have been created. So a message is sent from the creator process to the properties that are assessed, the one that belongs to this session, and a message is provided there indicating that the new process and thread have been created.

	So at this point, from the kernel's perspective, the process creation has succeeded, and the CreateProcessAPI will return true or a successful status to its caller. However, at this point, some more initialization has to happen as part of the loader, which is part of NTDLL's implementation.

	So what are the things that are done by NTDLL at this point? So one thing is creation of this usable data structure we'll take a look at later on this module as well called the Process Environment Block for the process and the Thread Environment Block for the first thread in the first thread in the process. We've seen a few details of the Thread Environment Block in the previous course. And then it also performs some extra intializations which are needed for code to be able to do stuff, such as creating a default heap, the thread pool, and some other initializations that are necessary at this point. And notice this is way before the code in our main function is actually running, so lots of things are actually happening even before our main function is running.

	The next step for NTDLL, the loader, is to locate all the required DLLs as they are listed in the PE image, all these dependencies that are needed for execution of the process. And so the loader here locates all these DLLs. I'll describe later on in this module where it is actually looking for these DLLs. And for each one of them, it loads them. And of course, each DLL might have its own dependencies, so this is a recursive process, so to speak. So it might need to go into the PEs of these other DLLs and see what their dependencies are until all the dependencies are satisfied, meaning all the DLLs have been loaded. And then the next step is to call the DLL main function, which is a function every DLL can optionally implement. That would allow every DLL to initialize itself in an appropriate way.

	If anything fails here, if a DLL is failed to be located or if some DLL returns a failure status from its DLL main function, the process will terminate and will not continue with execution. But assuming everything goes as planned and all the DLLs, the dependencies are loaded correctly and initialized correctly, then the entry point coming from the portable executable is going to be executed, and that entry point is usually coming from the C runtime. It's not really our main function. It's actually the C runtime, and that C runtime is going to call our main function, and then our real code that we have perhaps written for that particular executable is actually going to be executed.

	So you might be wondering, what's the purpose of the C runtime anyways, and I'll explain later on in this module.


*** DLL Search Order ***
	We talked about the act that the loader, once it starts running code in the new process, one of the last things it does is to load all the dependencies for the executable, the ones that can be read from the PE file itself. And so the information the PE file only contains names of DLLs and all the symbols that are imported or used by the process, or by the executable to be more precise, and the names of the DLLs are just the file names, there's no full path there. And this is intentional. And now it's the job of the loader to figure out where those files are and uses a certain search order. So it just has the file name itself, like kernel32.dll, and it's up to the loader now to figure out where that file is, if any.

	So the first location to be searched, which is not really a search that is performed, and by the way this search order is the same that is done by calling the LoadLibrary function, which I'll demonstrate later in this module, if you just specify a file name, it goes throught the same search order. So the first location is the known DLLs. And so known DLLs is a set of DLLs that are listed in the registry in a certain key that are being mapped very early in the Windows boot to section objects which are used to map files. In this case, those PE files. And those always take precedence over any kind of file system search. And so it's not possible to spoof a process or executable by providing an alternative version of kernel32, for example, because it is part of the known DLLs list.

	And so the next thing, assuming that the DLL in question is not part of the known DLLs list, the next location to look for DLLs is the directory of the executable. That's the first location being searched in the file syste, and that should be the most common case for private DLLs used by the process that are needed by that executable and by processes created based on that executable. It makes it easy to move around the application just by copying all the files to a different directory, and that's why it's the preferred way or the preferred location to put DLLs that the executable needs.

	And then if the DLL is not found there, the next direction to be searched is the fairly well-known System32 directory that can also be obtained programmatically using the GetSystemDirectoryAPI, and this is where most of the Microsoft DLLs reside. And so System32 contains those Microsoft DLLs, and we typically don't want to put our own private DLLs in the System32 directory. That would be a bad idea. In fact, it would cause issues that even were given a name called DLL hell.

	What is DLL hell? And so suppose I have an Application A being installed, and that's using a DLL called x.dll, and that's putting that particular DLL in System32 because it knows it will always be found there. So you run Application A, and everyting seems to be working fine; however, next you might be installing application called B, which is a different application, but it is using another DLL that has the name x.dll. In fact, it could be the same kind of DLL, but perhaps a newer version. And if that application puts its version into System32, it would overwrite the previous x.dll. Now Application B runs just fine, but Application A fails to run or misbehaves in some way because compatibility might not be perfect between the second version and the previous version of x.dll. So what is the problem? The advantage of putting things in System32 is that they can be shared in physical memory, which of course saves RAM, which is nice; however, because of the DLL hell problem, this is not considered a good practice anymore. And even if you might have these two x.dll files which might happen to be the same with exactly the same version stored in different directories, each one in the current directory or the executable directory of those applications, they will not be shared in physical memory because they're coming from different paths and Windows will not compare those files byte by byte for sharingpurposes.

	So you might be wasting a bit more RAM, but because RAM sizes today are much bigger than they used to be in the past, it's considered still a better approach for stability and robustness.

	So System32 really should be used just for Microsoft's own binaries. So next, if the delay in question is not found there, the next location to be searched is the Windows directory, something like c:\Windows, which is the case for most systems. Again, this is part of the default search path. If it's not found even there, then the current directory of the process is being searched. This is a property of a process. You might recall from the CreateProcessAPI, the creator can provide its own or some current directory that's going to be used as part of this search path for the new process. Normally after the process is already live and well, it can change its own current directory by calling the SetCurrentDirectoryAPI, which affects further loading of other DLLs that are loaded explicitly, but for the purpose of just loading those dependencies when the process just starts up, there's no code in the executable that has a chance to run just yet, so the current directory is provided by the creator.

	You might be familiar with that in a different aspect if you create a shortcut, for example, like on the desktop, you can specify something called the working directory, and that working directory is the one that's going to be used by Windows Explorer when it calls the CreateProcess on your behalf when you double-click that shortcut. So this is where that current directory is coming from in case you're creating a shortcut. But in general, the creator has the power to provide that extra directory for search purposes for these dependencies.

	And finally, if that DLL is not found even in the current directory of the process, the last locations to search are all the paths that are listed in the path environment variable, which of course can contain multiple directories all searched in sequence, in order, until the DLL is found or not. And if it is not found, this is where the loader gives up and we get this nasty message box saying the code couldn't really move forward because the DLL was missing, it couldn't find this particular DLL. So in that case, the process shuts down and the DLL will terminate the process and that is it. So obviously all these dependencies have to be fulfilled in order for the process to make its way into the main function.

	And of course, it's not just about finding the DLLs themselves, it's also about running their DLL main function, which we'll look at in some more detail later on in this module so that DLLs can initialize themselves properly and they all have to report success in that initialization. If anyone reports a failure, then a different message box is going to be shown, but the final result is the same, the process is going to terminate.


*** DLL Implicit Linking ***
	What we've seen in the previous demo is that the executable has a bunch of dependencies which are written as part of the import inside the portable executable. This also applies to DLLs, which might depend on other DLLs. This is known as implicit linking to a DLL, and the way it's done from a developer perspective is by using a small LIB file that contains the name of the DLL without any path and a list of exported symbols. And these are the things that are going to be then embedded as part of the PE of the executable or the DLL that using or needs that other DLL. And so at runtime, the loader's purpose is to locate that DLL and all the other dependencies, of course, and to bind the real function implementation where they happen to fall.

	If the DLL is not found using the search path that we described earlier or the DllMain function of that DLL returns FALSE indicating that it failed to initialize for some reason, then the process would be terminated, which means that these DLLs are necessary, these are mandatory. The process can't really go forward without these DLLs existing.


*** DLL Explicit Linking ***
	The other option of linking with the DLL is known as explicit linking, sometimes called dynamic linking. Implicit linking that we've seen in the previous videos is sometimes also referred to as static linking, but we have to be careful there not to be confused with static linking to a static library, which is a different thing. And so for explicit linking, the idea here is that we don't want the DLL to be loaded up front when the executable starts its execution, we might want to load this DLL later on, and this is done using the LoadLibrary or LoadLibrary(Ex) APIs. In this case, you don't have to specify just the filename, you can also specify a full-blown path, and this will be the only location to be searched for that DLL. And, in the case of an error of the DLL is not found or if for whatever reason it's not properly initialized, then you get back a NULL handle and the process is not going to crash. It's now your perogative to handle that error in some meaningful way.

	This is very common for DLLs that hold onto resources that are based on some language. So resources could be string, icons, bitmaps, and so on. So you might have your application running somewhere in, let's say, France, and you might want to load the French related DLL that contains resources that are appropriate for France. If the DLL exists, great, you can use it, but if it doesn't, you might want to load a different DLL, maybe one based on English, which would be your fallback DLL. So you definitely don't want to cause that kind of strong dependency with that resource DLL that may or may not exist and it's not crucial to the running of the application. But it doesn't have to be resources, you can also use normal DLLs in the sense that it might contain code as well.

	So in this case, you can't just include some header file and then use or call the function directly. You can't do that because the linker will again complain that it doesn't find the function. You have to retrieve a pointer to the function directly, using the function GetProcAddress, which you can call after the DLL is explicitly loaded with the LoadLibrary or LoadLibrary(Ex) functions. This is only really viable with C functions, and we'll see that in a moment, that's because of that name mangling thing, because of that mangling, it's very difficult to attach to C++ functions and even more difficult to attach to C++ functions in C++ classes. And so, that's something you need to kind of take into consideration.

	There is in fact a way to have your cake and eat it too, this is known as Delay-Load DLLs. And the basic idea here is that you specify the DLLs at compile time, so at link time so to speak, but they're still loaded dynamically and not statically. This is something I will not cover but it does appear in the Microsoft documentation.

*** Process Termination ***
	There are several ways a process might terminate its execution. One option is that all the threads in the process exit. And we know that when the process remains without threads then it's essentially useless, and the kernel will terminate such a process, destroy that process object if possible. Another option is that one of the threads in the process calls the ExitProcessAPI from the Windows API. So it has to be done by a thread which is part of the process.

	And then the third option is a more violent option by calling the TerminalProcess API, which can be done from outside the process, but it requires a handle powerful enough to the process for this to actually work.

	And let me ask you a question. Which option do you think is the most common process to terminate? The answer is actually option number two, so that's calling the ExitProcess API. And you might find that a bit weird, I mean, isn't the first option perhaps more viable? But the thing is that most applications use the Windows subsystem, so the link against the C runtime that we talked about already. Now one of the things that happens with the C runtime is it calls your main function. And when the main function returns, the C runtime calls ExitProcess, which terminates the process entirely, killing all other threads that might exist in the process at that time.

	And so sometimes people refer to the first thread in the process as the main thread, as though it has some special powers. In fact, it doesn't. From the kernel's perspective, all threads in the process are the same, but from a practical perspective, when that first thread terminates when we return from the main function, then the process terminates because the C runtime is calling the ExitProcess API.


*** ExitProcess vs TerminateProcess ***
	First its important to list what is exactly the same between these two. In whatever way a process terminates whether a good way or bad way, whatever that way might be, there's nothing that's going to leak beyond the lifetime of the process, which means that the kernel guarantees that all the memory allocated will be released, the private memory, of course, and all the handles in the handle table are going to be closed. This is guaranteed, regardless of how the process ends its life.

	Specifically, ExitProcess can only be called by one of the threads inside that process, and this causes an orderly shutdown of the process. And by orderly, I mean that all the DLLs in the process get notified using their DllMain function that they're about to be unloaded from the process. And so they have the chance to perform any kind of clean-up or whatever they need to do, such as write something to a log or close some files, whatever they need to do, they get a chance to do that before the process finally terminates.

	With the TerminateProcess call on the other hand this can be done by any thread, theoretically. Usually it's done from a thread outside the process, and its purpose is to kill the process right here, right now. And so in this case, assuming this succeeds, assuming the thread gets a powerful enough handle to do that, as we've seen in the demonstration, then the process dies immediately, and the DllMain functions of all the DLLs currently loaded in the process are not going to be executed, which means that you might lose some data, as DLLs are not able to perform proper cleanup. It's not about freeing memory or anything like that, because that's going to happen anyway, but it's about certain operations that might want to do things such as write something to a file, and if they don't close the handle appropriately, you might get some data loss because the file has not been properly flashed and so on. So obviously, this is supposed to be a last resort kind of thing, and sometimes we do use that.

	If you look at Task Manager, there's the End Task button in the Details tab, that's exactly what it does. It tries to open the handle with the PROCESS_TERMINATE access mask to the process, and if successful, calls the TerminateProcess API. Here's the DllMain function. (Go back in the video and look)



*** Process-Related Data Structures ***
	Most of them are in kernel space, but there's at least one in user space, which the kernel doesn't really care about too much, but still something that is worth knowing about.

	So first, in the kernel, there is a data structure called EPROCESS, short for executive process, which represents and handles and manages everything there is to know about that process. It has many members, some of them have the information directly within them, some of them are pointers to other data structures that contain the rest of the information, but everything about the process can be obtained by starting from the EPROCESS data structure. The first member of an EPROCESS data structure is of type KPROCESS, short for kernel process. You can think of that as having these two layers that we've seen in the previous course where we had this kernel and executive two layers, at least logical layers in the kernel.

	Second thing of that is similar to the way the EPROCESS is built, it has a KPROCESS structure which is mostly interesting for the kernel, the lower-level kernel, that handles things such as thread scheduling, and then there's the other parts of the structure of the EPROCESS, which contain the rest of the information, And so, the one thing that is always guaranteed is that the first member of an EPROCESS is of type KPROCESS, and has the name PCB, short for Process Control Block. In general, the EPROCESS stucture is not properly documented in the Windows Driver Kit or anywhere else in an official capacity, and its members sometimes move around, and that's why we really rely on the layout of the structure, although it is fully available in the symbols, as we'll soon see. The only thing we can rely on is that the first member of an EPROCESS is always the KPROCESS, so essentially, they're both in the same address.

	In addition all the processes in the system are managed in a doubly-linked list, and I'll show you this list in the next demo, but all the processes are there, listed in this circular linked list, and it's possible to perform some kind of iteration if some code desires to do that. In fact, every process enumeration function that you may or may not be familiar with eventually does that kind of iteration to figure out which processes exist in the system. And we'll see that the chain is actually managed by a data structure called LIST_ENTRY, and the member within the EPROCESS that chains the next and previous process is called ActiveProcessLinks. The root of this linked list is a kernel variable called PsActiveProcessHead, which we can get access to as well.

	And, in user mode there's another data structure that is often used, which is called the Process Environment Block, or PEB, which is yet another undocumented data structure and mostly the members there are not really provided officially by Microsoft, but this structure has been reverse engineered quite a bit, so we pretty much know what it actually holds, and some small parts of it are in fact lightly documented within the Microsoft documentation somewhere.


*** Protected Processes ***
	Protected processes started their life in the Vista timeframe, and the basic idea was to protect processes from invasive access by any user mode process, including processes running with admin privileges. The basic reason for that was because of the idea of the need to protect digital rights management content. At the time, there were lots of CDs and DVDs created that contained audio and video, and even though these could be encrypted by the companies creating the media, once it has to be played, then, of course, that content had to be decompressed or decrypted before being sent to the actual playback device. And once that's done in memory of the appropriate process, someone with admin privileges or sometimes even not with admin privileges could open an invasive handle to the process address space using the PROCESS_VM_READ access mask and then just grab the content of the media in its unprotected, unencrypted form. And so Microsoft was asked to find some kind of solution for that, and protected processes were created exactly for this purpose.

	With a protected process, there's no way to open a handle to the process with invasive access. You can only open the handle with very limited, shallow, superficial information you can get for the process, such as the process name, its image path, its priority, things like that, but you can't open a handle that, for example, can look at the address space of the process or look at the list of DLLs inside the process, even if you had admin privileges.

	So this is kind of the key point here. From user mode, you can't open an invasive handle to a protected process regardless of which user you are, even if you have admin privileges on the machine. This was very different the previous XP days where an administrator from user mode can do essentially anything.

	So at the time, there were mostly three executables that were running as protected. One of them was audiodg, which is the Audo Device Graph that was used to decode audio. And then there was also the Media Foundation Protected Media Path process that was used by the Media Foundation engine to decode audio and video. And finally, there was this process called WerFaultSecure. You might be familiar Werfaultsecure.exe which, is an executable that is sometimes launched when some other process crashes, and then WerFault can grab a dump file or the memory snapshot of that process as it crashes and then send that to some server for analysis. Could be Microsoft server, could be something else. It's really based on configuration.

	So what would happen if audiodg or mfpmp crashed? The normal WerFault wouldn't be able to open a powerful enough handle to extract dump information from the crashing process. So this is why WerFaultSecure is created, and WerFaultSecure is running protected as well. And so the basic rule is that the protected process has unlimited access to another protected process, but you can't do that with normal processes, and that's why WerFaultSecure was necessary. And so that was the idea of protected processes, which was really used just for Microsoft binaries or very specific binaries that had to be signed in a very particular way and that couldn't be used by third party, someone outside of Microsoft. Fortunately, that model was later extended.



*** Protected Processes Light (PPL) ***
	The originial protected processes model was limited to these DRM(Digital Rights Management) protection aspects. However, in Windows 8.1, Microsoft decided to enhance the protected process model into something called Protected Processes Light, or PPL. And this was extended for two very important aspects. One of them is the fact that there were several supported levels of protection. So not all protected processes are now the same; some processes are more protected than others, meaning that a more protected process has full access to a less protected process, but, of course not vice versa.

	And the second thing that was allowed is now to have third-party anti-malware executables, usually user mode services eligible to be signed by Microsoft and running as protected. And this was important in order to protect these processes from malicious actors that might disable them. So in a very kind of simplistic way, the way most anti-malware solutions work is that they have at least two components. One of them is usually a kernel driver. Its purpose is to get details about things happening in the system, such as getting process and thread notifications, when processes gets created, or destroyed, and so on, and it usually passes that information to a user mode service that does the actual decision-making and performs some kind of logic to try to figure whether this activity is malicious or not. And so if you have some malware infiltrating the system it manages to get some privileges of an administrator, it can stop the service, or even terminate the service, which essentially renders the anti-malware solution as completely dead in the water despite the fact that we have a driver that still runs and continues to gather information it is useless if nobody is acting on the information.

	So now it is possible to sign anti-malware services as protected as well. Also, as a nice bonus, many of the system processes are PPL protected since Windows 8.1. Processes such as the Smss, the session manager, Csrss, the Windows subsystem process, Services, which is the service control manager, wininit, and some others that are protected kind of out of the box, which makes it much more difficult to infiltrate those and terminate them because they are also protected from termination.

	So what levels are supported in terms of PPL?

	Signer		Level		Description
	WinSystem	7		System and minimal processes
	WinTcb		6		Critical Windows components
	Windows		5		Important Windows components handling sensitive data
	Lsa		4		Lsass.exe (if configured to run protected)
	Antimalware	3		Antimalware service processes, including 3rd party
	CodeGen		2		.NET native code generation
	Authenticode	1		Hosting DRM content
	None		0		Not valid (no protection)


	Microsoft defined these 8 levels, where level 0 essentially means unprotected. The first lefvel, which is known as WinSystem, is applied to the most important processes in the system. This is the system process itself, which is protected in this way, and also minimal processes we'll discuss in a coming part in this module.

	The next level is called WinTcb. TCB is short for trusted computer base, which means that we're talking about components here which are essential to the Windows operating system. they are typically critical processes, such as Csrss, Smss, these are supposed to be signed with level 6.

	And then we have level 5, which is a bit less powerful for various Windows components that are handling sensistive data. One of them is the Windows Defender Advanced Threat Protection product that is available for enterprise. And then we have Lsass. And so Lsass is a very, well, kind of well-known process in Windows that is used for authentication. Once it authenticates a user, it stores a blob of data that represents that authentication, and so that blob of data can be used to authenticate automatically on other machines in the domain. And in the past, it was used using something called pass the hash, or pass the ticket attack, which essentially means that someone managed to grab that blob from Lsass address space and then just use it to automatically log in to other machines in the domain as that user. So this is a very dangerous and fairly common attack, at least it was in the past.

	And so you might be wondering why Lsass is not running protected? And if you look at a typical Windows system, you'll find that Lsass is not running protected because it seems to be a good idea. If it would be protected, that blob wouuld not be able to be extracted by anyone, including those running with admin privileges.

	And so the problem is that, well, first you can make Lsass run protected by changing a certain value in the registry, but by default it's not running protected because protected processes have that limitation. They can only load DLLs signed by Microsoft. And it makes sense because if you can force or persuade some protected process to load your DLL, then you're in very good shape. From a security perspective, you can do a lot of things and you are very well-protected. And because Lsass can be used to authenticate using custom mechanisms, such as fingerprints or eye retina or other custom ways, Lsass is able to load these DLLs known as authentication packages. And because authentication packages can be written by any vendor and not just Microsoft, Lsass is not running protected by default simply as a way to ensure compatibility with existing and future authentication mechanisms.

	But it is possible to change that to protected. I can say that in the Windows Phone 10 environment when Windows Phone existed, Lsass was running protected, and that's becasue the Windows Phone device is much more closed or less open to adding new authentication mechanisms. In fact, Microsoft has full control of that phone factor. So if Lsass does run potected, this is the level that it runs at.

	For anti-malware, this is the level used for third parties. And we'll see in the next demo that even Windows Defender as being. well, something that Microsoft provides, is actually using the same signer level as anti-malware, and this is level number 3, and this does not allow termination of these processes from user mode, no matter what. There's some other less interesting signer levels, such as processes that host or generate .NET native code, and Authenticode is like the classic level to use for DRM content, and 0 just means no protection at all, which is the default for all non-protected processes.

	So you might be wondering what is that level, or where exactly does it appear? And as we'll see in the next demo, it's going to be part of the EPROCESS data structure in the kernel, and this is the way to know which process is more protected than another process, and so on.

	If you combine that, we get this set of values that are actually stored in the EPROCESS data structure, and so we have these two values. One of them is the signer level between 0 and 7, and then there's the Protected or Protected Light model. So the Protected model is still there, it's not going away, and for Protected, the value is 2, and for Protected Light, the value of 1. If you combine those, you get some number, and the higher the number means the higher the protection, the more powerful the process, and as mentioned earlier, a more protected process can fully access a less protected process, can fully access a less protected process, but not vice versa. (Look at the illustration shown at the end of the video 7:29)



*** UWP Processes ***
	Universal Windows Platform
		- Previously known as Metro, Store, Modern...
		- Run on top of the Windows Runtime
		- Declare capabilities
		- Execute in an AppContainer
		- Resources managed by the Process State Manager and Process Lifetime Manager
			- PSM part of the DCOM launch service
			- PLM part of Explorer.exe(twinui.dll)
			- Suspend / resume / terminate



	These applications are typically the ones you download from the Microsoft Store and can be installed on multiple types of Windows form factors, not just the normal Windows, but also Windows Phone, the HoloLens device, even Xbox One. They are all built around the Windows Runtime. The Windows Runtime is a platform, and API, that was created to help build these kinds of applications in a universal way. And universal here, of course, is limited to Microsoft's universe.

	And one of the things about these processes is that they declare capabilities in order to do things, and it's very similar to the model you find in the mobile world. In the mobile world, if you got to a store that you're using, you always see something like the process or the application wants to use your microphone, your camera, to have access to your documents and things like that, and it's up to the user to decide whether the user is okay with that or not. If he or she is then they can download the application and get it installed. If not, then just don't do that.

	And so the way it's done in UWP is by using these capabilities, which are declared officially in something called an XML manifest, which is just submitted data that can be and , and fact, is necessary when building these kinds of applications. So we have these capabilities, and then they're going to be shown on the Store page of the application once it gets apporved. And using these capabilties, the application can do stuff. By the way, even with these capabilties, in many cases, the process doesn't have enough power to do the actual operations, and it needs a helper called the Runtime Broker, which is like a helper process that performs the operations once these capabilties are properly declared by the real application.

	So one of the properties of these processes is that in terms of security, they're fairly weak. And that should give users a good sense of safety when they download these applications, and they should feel okay becasue theese processes can't really hurt the system in almost no discernible way.

	And this is implemented using something called an AppContainer, which is a security sandbox of sorts. It's not a true sandbox, but it does provide some sandboxing capabilities.

	In addition, we've seen already in the previous course that when we do things like minimize such a window that belongs to one of these applications or processes, all the threads become suspended, so they get some kind of extra management by these two components known as the Process State Manager and the Process Lifetime Manager. And they provide two different services, so to speak, we have suspension and resumption. So this is something that we've seen already when the threads get suspended and such a process becomes minimized, or one of its windows becomes minimized and restored properly, the threads resume. But there's also another thing that could happen to these processes is that they could terminate at any given point in time without any special further notice, and this is done, because, for example, the system is very low on memory. If the system is very low on memory, the first things to go are UWP processes and that's because these processes are expected to save state often, so the next time they launch they get information saying, hey, you were launched because you might have been terminated previously or perhaps that's a new launch, and the application should do the right thing in this case.

	Termination can also occur because of some misbehavior by the applications, such as consuming too much memory. There are some limits that Windows imposes on UWP processes, so they don't consume too many resources.

	So obviously these are useful when you want to create applications that run on multiple form factors, but normal applications that are not UWP don't suffer, so to speak, from these limitations, but they cannot be run on any kind of Windows device without first doing proper checking. In fact, some of the normal applications, Win32 applications, are not officially supported on non-normal standard laptop or desktop-based systems.



*** Minimal Processes ***
	A really new kind of process has emerged in Windows 10 known as a minimal process. Now minimal processes were created based on a project within Microsoft Research called Drawbridge, and there's still videos that were created by the team, that you can find online. I recommend you watch them to get a better sense of what they are trying to do.

	The basic idea of minimal process, something we briefly discussed in the previous course, which is having an empty address space, and that address space can be used by the kernel in any way it sees fit. So there's no NTDLL, no subsystem DLLs, no PEB, no TEBs, no nothing, essentially something which is like an empty box of address space that the kernel can use in any way it sees fit.	

	What is the purpose of these kinds of processes? Obviously user-mode code can't really run in this kind of process and this kind of process cannot even be created by any user-mode code, because that code calling something like CreateProcess always has to provide some kind of executable, but there is no executable that's going to be mapped into such a process. So what's the real use of the minimal process?

	So basically this is the way for the kernel to use an address space to store data without having to use the kernel address space for that, because the kernel address space already has lots of entities there competing for memory, such as the various kernel drivers and the kernel itself, and so management of the pool is pretty difficult to do and sometimes it may be convenient to put memory that is used for a certain purpose in a different location that is easy to manage and highly protected, and this is exactly what minimal processes are about.

	And the canonical example is the memory compression process, which we've talked about in the previous course, which stores compressed memory, and that compressed memory is simply being manipulated by the kernel in order to compress and decompress the memory as required. Another process was created several versions later in Windows 10 that is also a minimal process is the registry process. And the registry process is used for exactly the same kind of concept, to store registry-related information. Instead of putting them in the page pool in normal kernel memory, so to speak, it's now put in its own process, which makes it just easier to manage and less contention happening with the normal kernel memory used by kernel drivers.

	This idea has been significantly extended using this thing called the pico process. A pico process is a minimal process with a supporting kernel driver, which is called the pico provider. In fact, a pico provider must be registered at boot time, it must be signed by Microsoft, and currently this is the only way to create a pico provider, and as I'll explain a bit later, it actually makes sense at this point. There can only be one pico provider on a system. This is a current limitation which may or may not be lifted in the future.

	And the basic idea is that if there are any system calls or any kind of things that require access for the kernel to be more precise, then the pico provider is going to intercept them. So because the pico provider is first and foremost a minimal process, and pico processes are a minimal process to begin with, so what would be the code running in the minimal process, because we know that normal Windows code can't really run in a minimal process, as there is no basic ingredients required for such processes, such as NTDLL. However pico processes are used for implementing the Windows Subsystem for Linux version 1.


*** Process Types ***
	Let's summarize the process types Windows supports. Before the existence of minimal processes, there was just one type of process. Let's call that NT process. This kind of process has the normal ingredients that any Windows process should have, such as NTDLL Thread Environment Block, Process Environment Blocks and something called Shared User Data, which is a data structure which is mapped to the same address in all processes, and, of course, subsystem DLLs and all these things. And when we talked about protected processes although they have some special properties, they're still normal process in that respect. They're still NT processes that just have some certain different behavior, perhaps, in terms of security.

	But minimal processes are really completely new. A minimal process is just an empty address space. That's it. It can only be manipulated by the kernel directly, and no system calls are required or possible, even, because there's no user mode code running there.

	The third option is the extension of the minimal process model, which is pico processes. Pico processes are minimal processes with a supporting pico provider, which is currently just one of those existing that provides the support for Linux version 1. And the basic idea here is that the Linux process is going to run here, and it's going to perform system calls that are Linux system calls. Linux system calls use the same syscall instruction, x64, for instance; however, the calling convention is different, the actual system calls themselves are different, so the pico provider is the one providing the Windows kernel support, so to speak, that is required to act as the Linux kernel.

	And the way this works is that the pico process has this bit in the EPROCESS in one of the flags saying pico process. And when the system service dispatcher in the Windows kernel gets the actual system call, it says who is the caller? If the caller is a pico process, it simply redirects it to the pico provider and says, hey, here you go, you handle that system call. Otherwise, it will handle the system call normally. So this is the way it works, and this is how WSL 1 is implemented.

	If we compoare WSL 1 to WSL 2, what are the main differences? WSL 1 is built on top of pico processes. These are necessary, and the pico provider is acting as the Linux kernel. User mode parts of the Linux kernel are used from a real distro, so these are normal Linux executables. The only thing that is slightly different is the init process, it has to know about the fact that it's part of running on Windows but other than that, all the normal ingredients from any Linux distribution are exactly this same. Running as is, starting with those binaries that have the ELF format rather than the PE used on Windows. So it's part of the pico provider's purpose here in WSL 1 to figure these things out and parse the ELF appropriately to actually launch the Linux process as expected. And so the kernel is really provided by the pico provider that's acting as the Linux kernel. The Linux kernel is not being used in WSL 1.

	This has some downsides. The performance in terms of I/O, in particular, is not ideal, not as fast as in a true Linux system. WSL 2 improves many of these things significantly by also allowing gui application, by utilizing various support in the X Window System that Linux knows how to work with. So the idea of pico processes, although very cool in my humble opinion, were dropped, so they're no longer being used. They're still supported because, as we've seen, we can switch between WSL 1 and WSL 2 at will, but they're not going to be developed any further, probably unless there's really some kind of a new idea about that. But if you think about the idea of a pico provider, it's actually very cool because you can think of implementing othere operating systems in terms of Windows, but that would require us to have access to the Windows source code because, in many cases, you have to call internal functions which are not exported, and I'm not even talking about the actual documentation.

	So maybe someday Microsoft will open this thing up and we'll be able to implement other operating systems in terms of Windows by writing our own pico provider. But that's perhaps just a pipe dream that may or may not ever happen.

	In WSL 2, a full Linux distribution is installed, user mode and kernel becasue, really the Linux kernel is the best one suited to handle system calls coming from Linux user mode processes. And the technology behind the scenes is called Hybrid Virtual Machine. It uses the Hyper-V hypervisor in order to make the appropriate field that it is the actual same system, but the Linux kernel doesn't really need to know that, so you can use the true Linux kernel.

	The actual way this works is not very well or deeply documented by Microsoft, but there are some articles that have been written by the Microsoft Kernel team, the windows kernel team, that describes some aspects of thsi hypbrid virtual machine technology, and I definitely recommend you read that, and there's lots of other esearch you can do in this space, obviously. So this hybrid virtual machine is the one being used behind the cover, and as we've seen in the demo, it seems like I was able to launch a WSL 2 machine without any issues. It does require the hypervisor, and if you are using a Windows 10 or 11 virtual machine, you might need to turn on nested virtualization.





*** Jobs ***
	A job is a kernel object that manages a set of one or more processes.

	A job is one kind of kernel object, and it manages a set of processes in some way. A job without processes is completely useless, so the basic notion of a job is the ability to impose limits on processes that are part of the job. A job automatically provides something called accounting information, which essentially means some kind of statistics that are being tracked for all the processes within the job without having to set anything in particular. So even if you don't impose any kind of limits on a job, then it's still very useful to get that accounting information, such as CPU consumption, I/O consumption, and things like that, without having to query each process individually. Another thing we can do with a job is associate something called an I/O completion port. An I/O completion port, in general, is a Windows kernel object that allows triggering some kind of notification that then allows some function to be run by a bunch of theads or one of the threads within a thread pool.

	I/O completion ports are typically used for handling asynchronous I/O operations, but they can also be used as a notification mechanism for things that are happening in a job. For example, if a limit gets violated or if a process gets created inside the job, then the I/O completion port can be triggered, and some code you register by waiting for I/O completion port can then execute. This is of course completely optional.

	Now once a process is assigned to a job, it can never get out. So this is a one way street, so to speak, and that makes sense because if the process could say, well, I don't really like this limit, let me just get away from this job, that wouldn't make much sense, and so it's something that you can't really do. Once a process enters a job, it's going to be in that job until the end of its life. Now what happens if a process inside the job creates another process? By default, that process gets into exactly the same job unless the creation function, the CreateProcess API, uses a flag called CreateProcess BREAKAWAY_FROM_JOB, which is a way to say, I would like to create a new process outside of my job. So this would work unless the job has explicitly forbidden getting out of it by child processes. But without any special flags the new process will be created in exactly the same job. 


*** Job Limits ***
	- Maximum processes in a job
	- CPU: Per job & per process limit; affinity, rate control, quantum (server)
	- Memory: minimum & maximum working set; process commit maximum
	- Network and I/O: maximum network bandwidth; maximum I/O rate, read & write bytes
	- UI: USER & GDI handles; clipboard access; exiting Windows; others

	What kind of limits can be applied to a job? First we have a maximum number of processes in a job. So assuming we have a process within a job that creates more processes, the number of processes can be limited so that not to many processes are created. If the process that is now created goes beyond the limit, it is immediately terminated.

	Another kind of limit is CPU-related limit, and there are several of those. One of them is the CPU running time per job or per process. It's about actual CPU time used for code. So it's not just about the life of a process, it's about actually running code within the process using CPU resources. And once the limit is reached, the process will terminate, unless there's an I/O completion port, and then the I/O completion port will be notified, and then the code running as part of that I/O completion port handling might want to perhaps extend that limit. There's also the ability to limit affinity. Affinity is about which processors can be used by the job.

	CPU rate control is another very nice limit that was created in Windows 8 that allows limiting the percentage of CPU consumption for the processes in the job. There's also a way to kind of tweak a little bit the quantum the time slice used for every thread within the job, but this is only available on server machines.

	There's also the ability to limit the amount of memory being used. Physical memory, that's the working set, you can specify a minimum and a maximum. You might think what's the point of specifying a minimum? This allows the system to maintain at least some memory, always in RAM, which makes it perhaps a bit faster to access memory within this job. You can also set a process commit maximum. That's the maximum memory you can allocate in the process within the job, regardless of whether it's currently in physical memory or not.

	There are also limits you can set for network bandwidth and I/O bandwidth, and some other details about I/O reads and writes. These kinds of limits have been introduced in Windows 10.

	And finally, there's some limits related to user interface stuff, such as some limits on user interface and GDI handles. You can limit the ability to copy and paste for example. So if you don't like allow copied, it means you cannot copy from within the job to any process outside the job. You can also limit paste in the same way. So this kind of gives you a very lightweight sandbox of sorts, at least to some extent. And there's some other similar limits that you can specify. Now, of course, generally you can use any number of limits that you want, you just need to call the SetInformationJobObject function, as we'll see later multiple number of times. So you can combine these things in any way you see fit, and it's also possible to lift limits later on. The controller of the job, the one that has a powerful handle to the job, can play with these limits as that someone sees fit.


*** Jobs API ***
	Here are the main functions to use. First we have the CreateJobObject function that allows us to create a new job object, or optionally, open JobObject handle to an existing job object, assuming we know the name of that job object. The OpenJobObject function only allows opening a handle to an existing job object by name. If it cannot find the job under that name, then the function simply fails.

	And then, once we have a handle to a job that we can control, we can call the AssignProcessToJobObject function to add one or more processes to the job. We'll have to call that multiple times, each time with a different process handle if we need more than one.

	And then, the super function really that provides a way to set up all the various limits is SetInformationJobObject. That function has been extended throughout the years to support more and more job features. As we'll see in the next video, it's about providing the appropriate enumeration value and an associated data structure that contains the details of the limit.

	The opposite function is QueryInformationJobObject, that allows us to query existing limits, to query the list of process in the job, and query the accounting information that the job maintains, regardless of any specific limits.

	And finally there's the convenience function called TerminateJobObject, that terminates all the processes in the job. It is as though you would call the terminate process function for each specific process in the job, so this is kind of a shortcut you can take in order to terminate all the processes in a job in one stroke. Note that after such a call, the job is still very much alive, it simply contains currently, 0 processes.  


*** Nested Jobs ***
	So when jobs were first created, in fact, up until Windows 8, you could assign a process to a single job only, and this provides a very frustrating limit at times because if you have a process you'd like to control in some way by putting it into one job that you control, and that process happens to be already part of a job, you're out of luck, there's nothing you can do about it because we know that a process, once it enters the job, it can never leave that job. So if you try to assign a process to a second job, this would simply fail.

	In Windows 8 and later, this limitation was lifted. So now, you can create the process or take an existing process and have that process assigned to more than one job. This will create a job hierarhy, if this is possible. There are some cases which this will also fail, but these cases should be fairly rare. So now it's possible to add a process to more than one job. In one sense, this was kind of necessary because UWP processes, that we looked at earlier in this course, are always part of a job, and we know that the job uses the Deep Freeze functionality to suspend the proces. And so it would be very kind of problematic if you could just use that particular job for the process and you couldn't add the process to a different job that had, perhaps, different limits you might want to apply.

	And, in case that we have this nested job hierarchy, the only rule is that a parent job has more power than a child job. So in case of conflict, parent job always wins out. So essentially, a child job can create something which is more restrictive, but not more permissive than its parent job, that's the only requirement, and if you try to go against the requirement, simply, the limit you provide will be ignored if they go against or become more permissive than the parent job provided.

	So here is an example of how nested jobs get created. Here we have a Job A, currently without any processes, so let's say I'm adding process P1 to Job A. So now process P1 is affected by Job A only. Now we're adding process P1 to a second job, Job B. So what happens here, assuming Job B was just a separate job previously, Job B becomes a child of Job A, and a hierarchy is now formed. So now P1 is affected by Job A and by Job B, whatever limits are applied, and in case of a conflict, the more restrictive the limits that are applied. And then suppose I'm adding process P2 to Job A, the as usual, process P2 is now affected by Job A. If I add process P2 to some Job C, then again the hierarchy, continues to be built, and now Job A has two child jobs, Job B and Job C. And P2, as before, is now affected by Job C and Job A.

	So here's the interesting part, let's say I'm adding process P3 to Job B only. I'm not necessarily aware of Job A. Regardless, because of the existing hierarchy, P3 is going to be affected by Job A and Job B, even though P3 perhaps wasn't put into Job A explicitly, and the one that is controlling Job B is not necessarily aware of Job A. P3, regardless, is also affected by the limits provided by Job A and Job B.



*** VMs vs Containers ***
	Virtual machines are more resource intensive than containers

	Containers are less secure than VMs because they are not as separated from the system as a VM
	
	Containerized applications should be as isolated as possible
		- Isolated file system
		- Isolated registry
		- Isolated object namespace

	Windows 10 version 1607+ achieves this with Silos
		- Required depp changes within executive components


	Silos are essentially a super job because a job already contains a bunch of processes, but it is not a full-blown sandbox. A silo provides extra ways to provide this kind of isolation that we need and a sandbox of sorts. And this required deep changes within executive components at that time because if now we have a call to some function that tries to create an object that has a name, for instance abc, then the executive has to ask, well, where are you coming from, from which container are you, and then provide the correct redirection to the correct object namespace. The same goes for file system operations and registry operations.

	Every silo starts its life as a job. And then there's a call to the SetInformationJobObject that makes this job upgrade to something called a silo, which now has containment capabilities, as we'll soon see.


*** Silos ***
	There are basically two types of silos that are supported. One type is called an application silo. This is the less interesting silo for our purposes. This provides some kind of file system redirection for desktop applications that have been converted to UWP using a technology called the Desktop Bridge. So you might recall, we were using WinDbg Preview. WinDbg Preview can be downloaded from the Microsoft store. You might presume that this is a UWP application, but, in fact, it is not. It is actually a normal Win32 application based on WPF, the Windows Presentation Foundation technology from .NET, which is a normal application that doesn't adhere to the UWP model in fact. However, it has been converted using the Desktop Bridge technology to UWP in terms of the way it is deployed and provided to the store. And this provides an easy way to update the debugger whenever new updates are available.

	And you may have noticed in this course we used that multiple times. We ran the debugger with admin privileges, something that you normally cannot do for UWP application. So, the debugger is running in an application silo, which provides some very simple file system redirection for temporary files. But other than that, it has no kind of containerization capabilities or properties, so if the debugger goes to something like c:\Temp, it is the real c:\Temp on the host. So this is not what we care about here.

	The true containers are server silos. Server silos are the ones used to implement Windows containers by providing isolation for file system, registry, and object namespace. And that allows us to create true containers. This the real silo this is the thing that allows us to create Windows-based containers.

	So here is the general server silo architecture. This is what it looks like when we want to build containers. And we typically use Docker to do that or other technologies built on Docker. Docker started its life in the Linux world, but then they also wanted to move to Windows and Windows can use Docker to host containers using silos.

	
